{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756d816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 1.9.0+cu111)\n",
      "    Python  3.8.16 (you have 3.8.5)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from egg.zoo.pop.utils import get_common_opts, metadata_opener, load_from_checkpoint\n",
    "from egg.zoo.pop.games import build_game\n",
    "import hub\n",
    "from torchvision import transforms\n",
    "\n",
    "#from egg.zoo.pop.data import ImageTransformation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326e031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/mmahaut/projects/exps/tmlr/vocab_64/590132/final.tar\"\n",
    "metadata_path = \"/home/mmahaut/projects/exps/tmlr/vocab_64/590132/wandb/latest-run/files/wandb-metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1dbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(additional_receivers='[]', additional_senders='[]', augmentation_type=None, aux_loss=None, aux_loss_weight=0.0, base_checkpoint_path='', batch_size=64, block_com_layer=False, checkpoint_dir='/home/mmahaut/projects/exps/tmlr/vocab_64', checkpoint_freq=0, com_channel='continuous', continuous_com=True, cuda=True, dataset_dir='/projects/colt/ILSVRC2012/ILSVRC2012_img_val/', dataset_name='imagenet_val', device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, force_gumbel=False, fp16=False, gs_temperature=5.0, gs_temperature_decay=1.0, image_size=384, is_single_class_batch=False, keep_classification_layer=False, load_from_checkpoint=None, lr=0.0001, max_len=1, minimum_gs_temperature=1.0, n_epochs=25, no_cuda=False, noisy_channel=None, non_linearity='sigmoid', num_workers=4, optimizer='adam', path_to_kmeans=None, pdb=False, preemptable=False, random_seed=111, recv_hidden_dim=2048, recv_temperature=0.1, remove_auxlogits=False, retrain_vision=False, return_original_image=False, similbatch_training=False, simplicial_L=10, simplicial_temperature=1.0, straight_through=False, tensorboard=False, tensorboard_dir='runs/', test_time_augment=False, train_gs_temperature=False, update_freq=1, update_gs_temp_frequency=1, use_augmentations=False, use_different_architectures=True, use_larc=False, validation_freq=1, vision_model_name='', vision_model_names_recvs=\"['vit']\", vision_model_names_senders=\"['vit']\", vocab_size=64, weight_decay=1e-05)\n"
     ]
    }
   ],
   "source": [
    "opts = None\n",
    "with open(metadata_path) as f:\n",
    "    opts = get_common_opts(metadata_opener(f, data_type=\"wandb\", verbose=True))\n",
    "print(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1d4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize module vit\n",
      "['vit']\n",
      "['vit']\n",
      "# loading trainer state from /home/mmahaut/projects/exps/tmlr/vocab_64/590132/final.tar\n"
     ]
    }
   ],
   "source": [
    "pop_game = build_game(opts)\n",
    "load_from_checkpoint(pop_game, model_path)\n",
    "senders = pop_game.agents_loss_sampler.senders\n",
    "for sender in senders:\n",
    "    sender.eval()\n",
    "    # TODO : remove gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc771eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/places205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/places205 loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r",
      "\r",
      "\r"
     ]
    }
   ],
   "source": [
    "ds = hub.load(\"hub://activeloop/places205\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131275b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformation:\n",
    "    \"\"\"\n",
    "    resize and normalize\n",
    "    \"\"\"\n",
    "    def __init__(self, size: int):\n",
    "        transformations = [\n",
    "                transforms.Resize(size=(size, size)),\n",
    "            ]\n",
    "        transformations.append(\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            )\n",
    "        self.transform = transforms.Compose(transformations)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5439e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmahaut/.local/lib/python3.8/site-packages/deeplake/integrations/pytorch/common.py:126: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "size = 384\n",
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(int(3/x.shape[0]), 1, 1)),\n",
    "    transforms.Resize(size=(size, size)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return (\n",
    "        torch.stack([x[\"images\"] for x in batch], dim=0),\n",
    "        torch.stack([torch.Tensor(x[\"labels\"]).long() for x in batch], dim=0),\n",
    "        torch.stack([torch.Tensor(x[\"index\"]) for x in batch], dim=0),\n",
    "    )\n",
    "dataloader = ds.pytorch(num_workers = 0, shuffle = True, batch_size= 128, collate_fn=collate_fn, transform={'images':transformations,'labels':None, 'index':None})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20d89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "  def __init__(self, input_dim=2, output_dim=3):\n",
    "    super(LinearClassifier, self).__init__()\n",
    "    self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "classifiers = [LinearClassifier(16, 245).to(\"cuda\")] * len(senders) # keeping all classifiers on cpu\n",
    "classifiers.append(LinearClassifier(16, 245).to(\"cuda\")) # <-shuffled-classifier : input a random message from one of the senders\n",
    "\n",
    "optimizers = [torch.optim.Adam(classifier.parameters(), lr=0.01) for classifier in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "\n",
    "def forward_backward(model_idx, input_images, labels):\n",
    "    # everyone goes to selected device\n",
    "    senders[model_idx].to(device)\n",
    "    \n",
    "    message = senders[model_idx](input_images)\n",
    "    output = classifiers[model_idx](message)\n",
    "\n",
    "    loss = criterion(output, labels.view(-1))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizers[i].step()\n",
    "    optimizers[i].zero_grad()\n",
    "    \n",
    "    senders[model_idx].to(\"cpu\")\n",
    "\n",
    "    return message, output, loss\n",
    "\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    _rand_sender = torch.randint(0, len(senders), (1,)).item() # chosing the shuffled input for shuffled-classifier\n",
    "    images, labels, _ = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    for i in range(len(senders) - 1):\n",
    "        _, output, loss = forward_backward(i, images, labels)\n",
    "        if batch_idx % 1000 == 0 :\n",
    "            print(f\"acc_{i} : \", (labels.to(device) == output.argmax(dim=1)).float().mean())\n",
    "        if i == _rand_sender:\n",
    "            _, output, loss = forward_backward(-1, images, labels)\n",
    "            if batch_idx % 1000 == 0 :\n",
    "                print(\"acc_shuffled : \", (labels == output.argmax(dim=1)).float().mean())\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
