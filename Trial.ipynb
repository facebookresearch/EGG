{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import egg.core as core\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 10\n",
    "\n",
    "# For convenince and reproducibility, we set some EGG-level command line arguments here\n",
    "opts = core.init(params=['--random_seed=7', # will initialize numpy, torch, and python RNGs\n",
    "                         '--lr=1e-3',   # sets the learning rate for the selected optimizer \n",
    "                         '--batch_size=32',\n",
    "                         '--optimizer=adam'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs\n",
    "\n",
    "# function to make the one hot vectors for each i_att\n",
    "def generate_one_hot_vectors(x):\n",
    "    one_hot_vectors = []\n",
    "    for i in range(x):\n",
    "        vec = np.zeros(x)  # Create a zero vector of length x\n",
    "        vec[i] = 1         # Set the i-th position to 1\n",
    "        one_hot_vectors.append(vec)\n",
    "    return one_hot_vectors\n",
    "\n",
    "# item is concat of its attributes, create all possible combos\n",
    "def concat_one_hots(one_hot_vectors, num_attributes):\n",
    "    combinations = []\n",
    "    for combo in itertools.product(one_hot_vectors, one_hot_vectors):\n",
    "        combined = np.concatenate(combo)  # Concatenate the two one-hot vectors\n",
    "        combinations.append(combined)\n",
    "    return combinations\n",
    "\n",
    "\n",
    "def create_input(num_attributes, values):\n",
    "\n",
    "    # Creating the encoder\n",
    "    input = np.array([])\n",
    "\n",
    "    if num_attributes == 'Random':\n",
    "        num_attributes = np.random.randint(2, 5)\n",
    "    elif isinstance(num_attributes, int) == \"False\":\n",
    "        raise TypeError(\"Please enter an int for num_attributes.\")\n",
    "    elif num_attributes < 2 or num_attributes > 4:\n",
    "        raise ValueError(\"Please choose value for n_attributes >= 2, <=4.\")\n",
    "\n",
    "    if values == 'Random':\n",
    "        for attribute in range(num_attributes):\n",
    "            values = np.random.randint(4, 101)\n",
    "    elif isinstance(values, int) == \"False\":\n",
    "        raise TypeError(\"Please enter an int between 4 and 100 for values.\")\n",
    "    elif values < 4 or values > 100:\n",
    "        raise ValueError(\"Please choose an int  >=4, <=100.\")\n",
    "\n",
    "    one_hots = generate_one_hot_vectors(values)\n",
    "\n",
    "    results = concat_one_hots(one_hots, num_attributes)\n",
    "    size_i = values**(num_attributes)\n",
    "\n",
    "    \n",
    "    return results, size_i, values, num_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, size_data, n_values, n_attributes = (create_input(\"Random\", \"Random\"))\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "message_len = 10\n",
    "\n",
    "# shuffle the input data\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_label = []\n",
    "for i in range(len(dataset)):\n",
    "    data_with_label.append([dataset[i], i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from code compo_vs_generalization\n",
    "\n",
    "def split_train_test(dataset, p_hold_out=0.1, random_seed=7):\n",
    "\n",
    "    assert p_hold_out > 0\n",
    "    random_state = np.random.RandomState(seed=random_seed)\n",
    "\n",
    "    n = len(dataset)\n",
    "    permutation = random_state.permutation(n)\n",
    "\n",
    "    n_test = int(p_hold_out * n)\n",
    "\n",
    "    test = [dataset[i] for i in permutation[:n_test]]\n",
    "    train = [dataset[i] for i in permutation[n_test:]]\n",
    "    assert train and test\n",
    "\n",
    "    assert len(train) + len(test) == len(dataset)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(data_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tensor = torch.tensor(np.array(train[1]).tolist())\n",
    "#test_tensor = torch.tensor(np.array(test[1]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # create alphabet \\nalphabet_size = 100 \\n\\ndef create_alphabet(alphabet_size, seed):\\n    random.seed(seed)\\n    symbols = set()\\n    total_symbols = 63\\n    while len(symbols) != total_symbols:\\n        symbols.add(random.choice(string.ascii_letters + string.digits + \"#\"))\\n\\n\\n    if alphabet_size > total_symbols:\\n        alphabet = symbols.copy()\\n        while len(alphabet) != alphabet_size:\\n            alphabet.add(\\'\\'.join(random.sample(sorted(symbols), 2)))\\n\\n    return list(alphabet)\\n\\n\\nalphabet = create_alphabet(100, SEED) '"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # create alphabet \n",
    "alphabet_size = 100 \n",
    "\n",
    "def create_alphabet(alphabet_size, seed):\n",
    "    random.seed(seed)\n",
    "    symbols = set()\n",
    "    total_symbols = 63\n",
    "    while len(symbols) != total_symbols:\n",
    "        symbols.add(random.choice(string.ascii_letters + string.digits + \"#\"))\n",
    "\n",
    "\n",
    "    if alphabet_size > total_symbols:\n",
    "        alphabet = symbols.copy()\n",
    "        while len(alphabet) != alphabet_size:\n",
    "            alphabet.add(''.join(random.sample(sorted(symbols), 2)))\n",
    "\n",
    "    return list(alphabet)\n",
    "\n",
    "\n",
    "alphabet = create_alphabet(100, SEED) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, hidden_size_receiver, n_inputs):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size_receiver, n_inputs)\n",
    "  \n",
    "    def forward(self, rnn_output, _input = None):\n",
    "        return self.fc(rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters not sure for all of them what they do\n",
    "n_output = 50\n",
    "n_input = len(train)\n",
    "max_len = 10\n",
    "cell = 'lstm'\n",
    "hidden_size = 142\n",
    "embed_dim = 5\n",
    "vocab_size = 10\n",
    "temp = 1.0\n",
    "hidden_size_receiver = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = nn.Linear(n_input, hidden_size)\n",
    "sender = core.RnnSenderGS(sender, vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size, max_len=max_len, temperature=temp, cell=cell)\n",
    "\n",
    "receiver = core.RnnReceiverGS(Receiver(hidden_size_receiver, n_input), vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size_receiver, cell='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(sender_input, _message, _receiver_input, receiver_output, labels):\n",
    "    return (sender_input - receiver_output).pow(2.0).mean(dim=1), {'aux' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = core.SenderReceiverRnnGS(sender, receiver, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "        {'params': game.sender.parameters(), 'lr': 1e-3},\n",
    "        {'params': game.receiver.parameters(), 'lr': 1e-2}\n",
    "    ])\n",
    "callbacks = [core.ConsoleLogger(as_json=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.label = data[0][1]\n",
    "        self.data = torch.tensor(np.array([data[0][0]]).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Return number of rows (lists)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label  # Return the row (list) at the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data):\n",
    "    _, labels = zip(*data)\n",
    "    n_ftrs = len(data[0][0])\n",
    "    features = torch.zeros(n_ftrs, n_ftrs)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        j = len(data[i][0])\n",
    "        features[i] = data[i][0]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_dataset = TensorDataset(train)\n",
    "test_tensor_dataset = TensorDataset(test)\n",
    "\n",
    "test_loader = DataLoader(test_tensor_dataset, batch_size=32, collate_fn=collate, shuffle=False)\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=32,collate_fn=collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[377], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mTrainer(game\u001b[38;5;241m=\u001b[39mgame, optimizer\u001b[38;5;241m=\u001b[39moptimizer, train_data\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m      2\u001b[0m                            validation_data\u001b[38;5;241m=\u001b[39mtest_loader[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m15\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer = core.Trainer(game=game, optimizer=optimizer, train_data=train_loader,\n",
    "                           validation_data=test_loader)\n",
    "trainer.train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # parts adapted from compo_vs_generalization, parts adjusted for this case\n",
    "class DiffLoss(torch.nn.Module):\n",
    "    def __init__(self, n_attributes, n_values,\n",
    "                 generalization=False):\n",
    "        super().__init__()\n",
    "        self.n_attributes = n_attributes\n",
    "        self.n_values = n_values\n",
    "        self.alphabet = alphabet\n",
    "        self.message_len = message_len\n",
    "        self.test_generalization = generalization\n",
    "\n",
    "    def forward(self, sender_input, _message, _receiver_input, receiver_output, labels):\n",
    "        batch_size = sender_input.size(0)\n",
    "        sender_input = sender_input.view(\n",
    "            batch_size, self.n_attributes, self.n_values)\n",
    "        receiver_output = receiver_output.view(\n",
    "            batch_size, self.n_attributes, self.n_values)\n",
    "        \n",
    "        acc = (torch.sum((receiver_output.argmax(dim=-1) == sender_input.argmax(dim=-1)\n",
    "                              ).detach(), dim=1) == self.n_attributes).float().mean()\n",
    "        acc_or = (receiver_output.argmax(dim=-1) ==\n",
    "                      sender_input.argmax(dim=-1)).float().mean()\n",
    "        \n",
    "        flattened_tensor = torch.flatten(sender_input[0])\n",
    "        probabilities = F.softmax(flattened_tensor / 1.0, dim=0)\n",
    "        sampled_idx = torch.multinomial(probabilities, message_len)\n",
    "        labels = ''\n",
    "        for char in range(len(sampled_idx)):\n",
    "            sampled_char = alphabet[sampled_idx[char]]\n",
    "            labels += sampled_char\n",
    "        loss = F.cross_entropy(receiver_output, labels, reduction=\"none\").view(\n",
    "                batch_size, self.n_attributes).mean(dim=-1)\n",
    "\n",
    "        return loss, {'acc': acc, 'acc_or': acc_or}\n",
    "        \n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #loss = DiffLoss(n_attributes, n_values)\n",
    "\n",
    "def loss(sender_input, _message, _receiver_input, receiver_output, _labels):\n",
    "    return F.mse_loss(sender_input, receiver_output, reduction='none').mean(dim=1), {'aux': 5.0} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train_loader = DataLoader(train, batch_size=alphabet_size)\n",
    "test_loader = DataLoader(test, batch_size=alphabet_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_labels = []\n",
    "#for item in train_loader:\n",
    "    #output_sender = sender(item[0][0])\n",
    "    #flattened_tensor = torch.flatten(output_sender[0])\n",
    "    #probabilities = F.softmax(flattened_tensor / 1.0, dim=0)\n",
    "    #sampled_idx = torch.multinomial(probabilities, message_len, replacement=True)\n",
    "    #label = ''\n",
    "    #for char in range(len(sampled_idx)):\n",
    "        #sampled_char = alphabet[sampled_idx[char]]\n",
    "        #label += sampled_char\n",
    "    #trainer_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" game = core.SenderReceiverRnnReinforce(sender, receiver, loss, sender_entropy_coeff=0.0, receiver_entropy_coeff=0.0)\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, optimizer=optimizer, train_data=train_loader,\n",
    "    validation_data=test_loader\n",
    "    ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" n_epochs = 15\n",
    "\n",
    "input = torch.zeros((hidden_size, max_len)).normal_()\n",
    "optimized_loss, aux_info = game(train, labels=None)\n",
    "\n",
    "trainer.train(n_epochs) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
